{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of weather on Yelp star rating\n",
    "\n",
    "It may be a good idea to segregate the data by business type (restaurant, hardware store, etc.). It could be easier and less computationally intensive per category. But it would be interesting to find very general features that can help determine star ratings for all businesses.\n",
    "\n",
    "Three stages: first we look at comment text alone, to see how accurate we can predict star rating based on that. Then we add in weather effects. Since star ratings are highly subjective, users may be influenced by many things when it comes to the rating. These factors will undoubtedly affect the review text as well, but there may also be subtle additional effects on the star rating.\n",
    "\n",
    "The goal isn't so much to painstakingly tune a NN for the last bit of accuracy, but rather to see if adding two new features can have a significant improvement regardless of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Dropout, Input, LSTM, Activation, Flatten,\n",
    "                          Convolution1D, MaxPooling1D, Bidirectional,\n",
    "                         GlobalMaxPooling1D, Embedding, BatchNormalization,\n",
    "                         SpatialDropout1D)\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/d/data/yelpdata/dataset/\"\n",
    "WEAT = f'{PATH}processed_weather/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#businesses = pd.read_csv(f'{PATH}business_on.csv', index_col=0)\n",
    "reviews = pd.read_csv(f'{PATH}review_on.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = reviews[['stars','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews['text'].fillna('empty', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Predicting star rating based on review text alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up(t):\n",
    "    t = t.strip().lower()\n",
    "    words = t.split()\n",
    "    \n",
    "    # first get rid of the stopwords, or a lemmatized stopword might not\n",
    "    # be recognized as a stopword\n",
    "    \n",
    "    imp_words = ' '.join(w for w in words if w not in set(stopwords.words('english')))\n",
    "\n",
    "    # lemmatize based on adjectives (J), verbs (V), nouns (N) and adverbs (R) to\n",
    "    # return only the base words (as opposed to stemming which can return\n",
    "    # non-words). e.g. ponies -> poni with stemming, and pony with lemmatizing\n",
    "    \n",
    "    final_words = ''\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    for (w,tag) in pos_tag(word_tokenize(imp_words)):\n",
    "        if tag.startswith('J'):\n",
    "            final_words += ' '+ lemma.lemmatize(w, pos='a')\n",
    "        elif tag.startswith('V'):\n",
    "            final_words += ' '+ lemma.lemmatize(w, pos='v')\n",
    "        elif tag.startswith('N'):\n",
    "            final_words += ' '+ lemma.lemmatize(w, pos='n')\n",
    "        elif tag.startswith('R'):\n",
    "            final_words += ' '+ lemma.lemmatize(w, pos='r')\n",
    "        else:\n",
    "            final_words += ' '+ w\n",
    "    \n",
    "    return final_words\n",
    "\n",
    "# what a great name. do_stuff\n",
    "\n",
    "def do_stuff (df):\n",
    "    text = df['text'].copy()\n",
    "    \n",
    "    text.replace(to_replace={r'[^\\x00-\\x7F]':' '},inplace=True,regex=True)\n",
    "    text.replace(to_replace={r'[^a-zA-Z]': ' '},inplace=True,regex=True)\n",
    "    \n",
    "    # Then lower case, tokenize and lemmatize\n",
    "\n",
    "    # with over 600,000 entries, this is going to be one hell of a long apply...\n",
    "    \n",
    "    text = text.apply(lambda t:clean_up(t))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converging to a very conventional convolutional NN model to convert non-conversational text to star rations\n",
    "# uh... with a non-convex loss function\n",
    "def cnn_model (X_train, y_train, test, val='no'):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(50000,128,input_length=1000))\n",
    "    model.add(Convolution1D(128,5,activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Convolution1D(128,5,activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Convolution1D(128,5,activation='relu'))\n",
    "    model.add(MaxPooling1D(35))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(5,activation='softmax'))\n",
    "    \n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9)     \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "    \n",
    "    if val == 'no':\n",
    "        model.fit(X_train,y_train,batch_size=128,epochs=5)\n",
    "    else:\n",
    "        model.fit(X_train,y_train,batch_size=128,epochs=5,validation_split=0.2)\n",
    "    pred = model.predict(test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = do_stuff(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_csv(f'{PATH}review_on_processed_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.Series.from_csv(f'{PATH}review_on_processed_text.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars = reviews['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    4\n",
       "2    3\n",
       "3    5\n",
       "4    4\n",
       "5    3\n",
       "6    1\n",
       "7    3\n",
       "8    5\n",
       "9    1\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "enc.fit(stars)\n",
    "y = enc.transform(stars)\n",
    "dummy_y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.fillna('empty', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words=50000)\n",
    "tok.fit_on_texts(data)\n",
    "     \n",
    "# set our max text length to 1000 characters, some of these reviews are pretty long\n",
    "sequenced = tok.texts_to_sequences(data)\n",
    "padded = pad_sequences(sequenced,maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded, dummy_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 405993 samples, validate on 101499 samples\n",
      "Epoch 1/5\n",
      "405993/405993 [==============================] - 481s 1ms/step - loss: 1.1001 - acc: 0.5063 - val_loss: 0.9555 - val_acc: 0.5742\n",
      "Epoch 2/5\n",
      "405993/405993 [==============================] - 483s 1ms/step - loss: 0.9658 - acc: 0.5713 - val_loss: 0.9317 - val_acc: 0.5861\n",
      "Epoch 3/5\n",
      "405993/405993 [==============================] - 478s 1ms/step - loss: 0.9395 - acc: 0.5825 - val_loss: 0.9223 - val_acc: 0.5918\n",
      "Epoch 4/5\n",
      "405993/405993 [==============================] - 486s 1ms/step - loss: 0.9245 - acc: 0.5899 - val_loss: 0.9236 - val_acc: 0.5993\n",
      "Epoch 5/5\n",
      "405993/405993 [==============================] - 485s 1ms/step - loss: 0.9127 - acc: 0.5950 - val_loss: 0.9198 - val_acc: 0.5923\n"
     ]
    }
   ],
   "source": [
    "pred2 = cnn_model (X_train, y_train, X_test, val='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 5 epochs it seems to be slightly overtrained, but for my purposes here I'm not too worried about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8725433443648803"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds2 = np.argmax(pred2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.69      0.71     15267\n",
      "          1       0.46      0.31      0.37     13016\n",
      "          2       0.49      0.50      0.49     22118\n",
      "          3       0.53      0.65      0.59     39190\n",
      "          4       0.70      0.65      0.67     37283\n",
      "\n",
      "avg / total       0.59      0.59      0.59    126874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ys,preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10511,  2899,  1035,   519,   303],\n",
       "       [ 2386,  4077,  5080,  1213,   260],\n",
       "       [  662,  1520, 10966,  8172,   798],\n",
       "       [  312,   216,  4539, 25325,  8798],\n",
       "       [  305,    59,   642, 12139, 24138]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix (ys, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While 0.59 precision isn't great, the 0.8725 AUC score is actually very very okay, one of the better kinds of okay. Also, the validation scores during training were very good, which is always helpful. A benefit, no doubt, of using all 630,000 reviews of business in the Toronto area.\n",
    "\n",
    "The vast majority of predicted scores are within 1 star of the actual rating. Additionally, 1 and 5 star ratings had the greatest precision and recall, so our model is decent at picking up extreme sentiment (or the users are effusive in praise and unrestrained in condemnation).\n",
    "\n",
    "But let's see if adding in weather and relative price can increase accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: weather effects\n",
    "\n",
    "Star ratings are neither objective nor scientific. We humans often make  bizarre, irrational and otherwise inconsistent choices due to many internal and external factors. Let's consider weather as one of the external factors, especially with regards to giving a star rating for a business. While good weather and a good mood might influence me to leave a more positive review as well as a higher star rating, there is really no way know the sort of review I would have left had the weather been different (the old problem of not knowing probabilities conditional on histories that haven't happened).\n",
    "\n",
    "What we can do is see if the review text matches with the score, and if knowing the weather conditions can improve the accuracy of our star predictions.\n",
    "\n",
    "This raises an interesting question for businesses, since weather is something entirely out of their control. But it does suggest that if a business' sales are clearly affected by its star ratings, perhaps something extra can be done to improve customer satisfaction on a rainy day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_w = pd.read_csv(f'{PATH}review_on.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_w = reviews_w[['stars','date','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv(f'{WEAT}all_weather.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['Year'] = weather['Year'].astype(int)\n",
    "weather['Month'] = weather['Month'].astype(int)\n",
    "weather['Day'] = weather['Day'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_w['date'] = pd.to_datetime(reviews_w['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
